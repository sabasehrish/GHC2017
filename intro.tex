\section{Introduction}
\label{sec:intro}
\squeezeup

%Experimental High Energy Physics (HEP) deals with the understanding of fundamental particles and the interactions between them.
%Experimental HEP is a compute- and data-intensive statistical science; a large number of interactions 
%must be analyzed to discover new particles or to measure the properties of known particles. 
%For example, data from over $300$ trillion ($3\times10^{14}$) proton-proton collisions at the Large Hadron Collider (LHC) were analyzed for the Higgs boson discovery.
 %approximately $1$ in $10^{13}$ LHC collisions yielded a distinguishable Higgs boson~\cite{higgsboson1}. 
%The size of the data sample (25 petabytes per year) required the use of a worldwide computing grid, 
%comprised of 170 computing facilities across 42 countries~\cite{lhcgrid}. 
%Future HEP experiments will bring in even more data, and processing and 
%analyzing will be more challenging. For example, while the LHC generates up to a billion collisions per second; the High Luminosity-LHC~\cite{hllhc} will generate 5 times this rate. These larger data samples will be needed to obtain a 
%deeper understanding of the Higgs boson and its implications for the fundamental laws of nature. 
 
Experimental High Energy Physics (HEP) is a compute- and data-intensive statistical science; 
a large number of interactions ($\approx$petabytes of data)
must be analyzed to discover new particles or to measure the properties of known particles. 
For example, data from over $300$ trillion ($3\times10^{14}$) proton-proton collisions were analyzed for the Higgs boson discovery at the 
Large Hadron Collider (LHC). 
The current LHC generates up to one billion collisions per second. When the High Luminosity-LHC~\cite{hllhc} turns on, it will generate 5 times this rate.
With this increase in data volume, the current tools and resources will be insufficient to 
provide low-latency interactive analysis. 

A typical complete HEP Analysis is an iterative process where low-latency and interactivity are key elements in scientific discovery. 
Analyses are IO bound and consist of several intermediate steps, each requiring storage of 
smaller reduced data sets on disk, which can be later accessed interactively for final analysis. 
These intermediate steps consist of applying selection algorithms, calculations of statistical summaries, 
and exploratory plotting of the relevant summaries. 
These steps can take anywhere from days to weeks. 
Storing intermediate results into files and then into data handling systems 
add to the processing time. 

%CMS example
In the CMS Dark Matter search for new types of elementary particles,
the data volume to be analyzed is about 200 terabytes of the 2015 dataset. 
 %and contains both data and Monte Carlo simulations. 
 This dataset is expected to significantly grow in the future as the HL-LHC approaches. 
 The data is reduced to couple of gigabytes to facilitate final analysis.  
Similarly, in an intensity frontier experiment, NOvA, a neutrino analysis selects 33 events (in megabytes range) out of a billion events (in terabytes range) for 
parameter estimation and model comparison studies. 

%a particular analysis 
 %performs selection on a billion events (in terabytes range), and uses the resulting 33 events (in megabytes range) with fewer 
 %attributes to do 
%The workflow starts with partly-reconstructed detector data,
%and with simulated neutrino events.
%These inputs come in the form of many files,
%with an aggregate size in the petabyte range.
%The first (reconstruction) step involves
%an \art framework program,
%run in many independent jobs,
%and is used to perform tasks such as particle identification
%and (neutrino) event identification.
%The output of these program runs is a large number of large n-tuple files,
%containing reduced data
%for each identified candidate neutrino event.
%The aggregate data size of the n-tuple files
%is in the terabyte range.
%The second (filtering) step
%involves a serial \cpp program,
%reads the large n-tuples,
%and performs a strict event selection
%and further reduces the data describing an event to a few numbers.
%The output of this filtering step
%is in the range of a few megabytes.
%The final (fitting) step consists
%of running a serial implementation
%of the Feldman-Cousins algorithm
%at each of many points in the mixing parameter space,
%chosen on a fixed grid,
%%to map out contour plots describing the correlated parameter estimates.
%\begin{figure*}[htbp]
%\begin{center}
% \includegraphics[scale=0.5]{analysisworkflow}
%\caption{Different steps in an analysis workflow and data sizes after each step }
%\label{fig:flow}
%\end{center}
%\end{figure*}
To 
improve time-to-physics and enable interactive analysis without significant overhead of bookkeeping and intermediate 
files, 
we need to explore alternate approaches such as in-memory data processing. The new large-scale, state-of-the-art computing systems provide tremendous compute and memory resources causing us to rethink the traditional methods of doing HEP data analytics.  
For example, Cori, a supercomputer at NERSC, provides total memory of 203 terabytes for the Haswell nodes with sustained application performance of 83 TFlop/s. 
However,  choosing programming abstractions to efficiently access and analyze data on these big compute machines remains a challenge. 
The traditional approach of using MPI on a system like Cori burdens the programmer with fine grain details and control of the application to utilize the system, such as data and task assignment, choice of memory technology, etc. Spark, an industry de-facto standard, the new approach of using high level programming abstractions on distributed datasets, has been recently made available on Cori. It is, however, neither designed for scientific applications nor to exploit the high performance computing platforms that are 
available to the scientific community.

In this talk, we present a thorough comparison of performance and usability aspects of 
both the traditional and the industry standard approach by implementing use cases from HEP.  
More specifically, we will focus on the challenges related to implementing selection and 
filtering criteria on billions of events using Apache Spark~\cite{spark,spark1} and MPI by running tests at NERSC~\cite{nersc-spark}. 
%Our work is unique because we have been using both MPI and Spark on HPC resources for HEP analyses. 